{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from scipy import misc, linalg\n",
    "from imp import reload\n",
    "from labfuns import *\n",
    "import random\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlParams(X, labels, W=None):\n",
    "    \"\"\"\n",
    "    param X: N x d matrix of N data points\n",
    "    param labels: N vector of class labels\n",
    "    return: mu - C x d matrix of class means (mu[i] - class i mean)\n",
    "    return: sigma - C x d x d matrix of class covariances (sigma[i] - class i sigma)\n",
    "    \"\"\"\n",
    "    assert(X.shape[0]==labels.shape[0])\n",
    "    Npts,Ndims = np.shape(X)\n",
    "    classes = np.unique(labels)\n",
    "    Nclasses = np.size(classes)\n",
    "\n",
    "    if W is None:\n",
    "        W = np.ones((Npts,1))/float(Npts)\n",
    "\n",
    "    mu = np.zeros((Nclasses,Ndims))\n",
    "    sigma = np.zeros((Nclasses,Ndims,Ndims))\n",
    "\n",
    "    for class_idx, class_ in enumerate(classes):\n",
    "        idx = np.where(labels==class_)[0] # Extract the indices for which y==class is true,\n",
    "        xlc = X[idx,:]\n",
    "        mu[class_idx] = xlc.sum(axis=0) / len(xlc)\n",
    "        for dim_idx in range(Ndims):\n",
    "            var = 0\n",
    "            for class_vect in xlc:\n",
    "                var += (class_vect[dim_idx] - mu[class_idx][dim_idx])**2\n",
    "            var += var / len(xlc)\n",
    "            sigma[class_idx, dim_idx, dim_idx] = var\n",
    "\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePrior(labels, W=None):\n",
    "    \"\"\"\n",
    "    param labels: N vector of class labels.\n",
    "    return: prior - C x 1 vector of class priors\n",
    "    \"\"\"\n",
    "    Npts = labels.shape[0]\n",
    "    if W is None:\n",
    "        W = np.ones((Npts,1))/Npts\n",
    "    else:\n",
    "        assert(W.shape[0] == Npts)\n",
    "    classes = np.unique(labels)\n",
    "    Nclasses = np.size(classes)\n",
    "\n",
    "    prior = np.zeros((Nclasses,1))\n",
    "    \n",
    "    for class_idx, class_ in enumerate(classes):\n",
    "        prior[class_idx] = np.where(labels==class_)[0].shape[0]/Npts\n",
    "\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyBayes(X, prior, mu, sigma):\n",
    "    \"\"\"\n",
    "    param X: N x d matrix of M data points\n",
    "    param prior: C x 1 matrix of class priors\n",
    "    param mu: C x d matrix of class means (mu[i] - class i mean)\n",
    "    param sigma: C x d x d matrix of class covariances (sigma[i] - class i sigma)\n",
    "    returns: h N vector of class predictions for test points\n",
    "    \"\"\"\n",
    "\n",
    "    Npts = X.shape[0]\n",
    "    Nclasses,Ndims = np.shape(mu)\n",
    "    logProb = np.zeros((Nclasses, Npts))   \n",
    "    \n",
    "    for data_idx, data_vector in enumerate(X):\n",
    "        prediction_vector = []\n",
    "        for class_idx, (class_prior, class_mu, class_sigma) in enumerate(zip(prior, mu, sigma)):\n",
    "            first_term = -0.5*(log(abs(class_sigma.diagonal().prod()))) \n",
    "            third_term = log(class_prior)\n",
    "            inverse_sum_term = 0\n",
    "            for vector_value, diag_value, mu_value in zip(data_vector, class_sigma.diagonal(), class_mu):\n",
    "                inverse_sum_term += (vector_value - mu_value)**2*(1/2*diag_value)\n",
    "            discriminant_function = first_term - inverse_sum_term + third_term\n",
    "            logProb[class_idx, data_idx] = discriminant_function\n",
    "\n",
    "    h = np.argmax(logProb,axis=0)\n",
    "          \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris, labels_iris, pcadim_iris = fetchDataset('iris')\n",
    "X_vowel, labels_vowel, pcadim_vowel = fetchDataset('vowel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_iris, sigma_iris = mlParams(X_iris, labels_iris)\n",
    "mu_vowel, sigma_vowel = mlParams(X_vowel, labels_vowel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_iris = computePrior(labels_iris)\n",
    "prior_vowel = computePrior(labels_vowel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_iris = classifyBayes(X_iris,prior_iris, mu_iris, sigma_iris)\n",
    "predictions_vowel = classifyBayes(X_vowel ,prior_vowel, mu_vowel, sigma_vowel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_accuracy(predictions, labels):\n",
    "    ground_truth_and_prediction_by_class = defaultdict(list)\n",
    "    correct_count_by_class = defaultdict(int)\n",
    "    prediction_and_ground_truth_by_class = defaultdict(list)\n",
    "    correct_count_by_prediction_class = defaultdict(int) \n",
    "\n",
    "    for prediction_idx, prediction in enumerate(predictions):\n",
    "    #ground_truth_and_prediction_by_class[ground_truth].append(predictions[ground_truth_idx])\n",
    "        prediction_and_ground_truth_by_class[prediction].append((prediction, labels[prediction_idx]))\n",
    "        if prediction == labels[prediction_idx]:\n",
    "            correct_count_by_prediction_class[prediction] += 1\n",
    "\n",
    "    for ground_truth_idx, ground_truth in enumerate(labels):\n",
    "    #ground_truth_and_prediction_by_class[ground_truth].append(predictions[ground_truth_idx])\n",
    "        ground_truth_and_prediction_by_class[ground_truth].append((ground_truth,predictions[ground_truth_idx]))\n",
    "        if ground_truth == predictions[ground_truth_idx]:\n",
    "            correct_count_by_class[ground_truth] += 1\n",
    "\n",
    "    print(correct_count_by_prediction_class)\n",
    "    #accuracy\n",
    "    for class_val, class_corr_count in correct_count_by_prediction_class.items():\n",
    "        print(class_corr_count / len(prediction_and_ground_truth_by_class[class_val]))\n",
    "    \n",
    "    print(correct_count_by_class)\n",
    "    #recall\n",
    "    for class_val, class_corr_count in correct_count_by_class.items():\n",
    "        print(class_corr_count / len(ground_truth_and_prediction_by_class[class_val]))\n",
    "\n",
    "#test==labels_iris\n",
    "#counter = Counter(test==labels_iris)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 50, 1: 42, 2: 37})\n",
      "0.9259259259259259\n",
      "0.7636363636363637\n",
      "0.9024390243902439\n",
      "defaultdict(<class 'int'>, {0: 50, 1: 42, 2: 37})\n",
      "1.0\n",
      "0.84\n",
      "0.74\n"
     ]
    }
   ],
   "source": [
    "# every time I predicted a 0 92% of the times was correct, 76% was correct when class 1 was predicted \n",
    "# 90% of the times class 2 was predicted was correct.\n",
    "recall_accuracy(predictions_iris, labels_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_and_prediction_by_class = defaultdict(list)\n",
    "correct_count_by_class = defaultdict(int)\n",
    "prediction_and_ground_truth_by_class = defaultdict(list)\n",
    "correct_count_by_prediction_class = defaultdict(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_idx, prediction in enumerate(predictions):\n",
    "    #ground_truth_and_prediction_by_class[ground_truth].append(predictions[ground_truth_idx])\n",
    "    prediction_and_ground_truth_by_class[prediction].append((prediction,labels_iris[prediction_idx]))\n",
    "    if prediction == labels_iris[prediction_idx]:\n",
    "        correct_count_by_prediction_class[prediction] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ground_truth_idx, ground_truth in enumerate(labels_iris):\n",
    "    #ground_truth_and_prediction_by_class[ground_truth].append(predictions[ground_truth_idx])\n",
    "    ground_truth_and_prediction_by_class[ground_truth].append((ground_truth,predictions[ground_truth_idx]))\n",
    "    if ground_truth == predictions[ground_truth_idx]:\n",
    "        correct_count_by_class[ground_truth] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 50, 1: 42, 2: 37})\n",
      "0.9259259259259259\n",
      "0.7636363636363637\n",
      "0.9024390243902439\n"
     ]
    }
   ],
   "source": [
    "#test==labels_iris\n",
    "#counter = Counter(test==labels_iris)\n",
    "print(correct_count_by_prediction_class)\n",
    "#accuracy\n",
    "for class_val, class_corr_count in correct_count_by_prediction_class.items():\n",
    "    print(class_corr_count / len(prediction_and_ground_truth_by_class[class_val]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 50, 1: 42, 2: 37})\n",
      "1.0\n",
      "0.84\n",
      "0.74\n"
     ]
    }
   ],
   "source": [
    "print(correct_count_by_class)\n",
    "#recall\n",
    "for class_val, class_corr_count in correct_count_by_class.items():\n",
    "    print(class_corr_count / len(ground_truth_and_prediction_by_class[class_val]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
